# T2 Neural Network Factor Selection

A PyTorch-based neural network system for Top-5 Factor Return Optimization. The system learns to identify which 5 factors from 83 available factors will deliver the highest returns, using historical forecast and actual return data.

## ğŸ¯ Overview

This system implements a feed-forward neural network that predicts factor returns and selects the top 5 performing factors each month. The model is trained using a custom loss function that maximizes the average return of the top 5 predicted factors.

## ğŸ—ï¸ Architecture

- **Model**: Feed-Forward Neural Network (83 â†’ 512 â†’ 256 â†’ 83)
- **Parameters**: 195,667 trainable parameters
- **Activation**: ReLU with 40% dropout
- **Loss Function**: Custom Top-5 Return Maximization
- **Optimizer**: Adam (lr=0.001, weight_decay=1e-5)
- **Training**: 60-month rolling windows with early stopping

## ğŸ“Š Performance Summary

- **Analysis Period**: February 2005 - March 2025 (242 months)
- **Annualized Return**: 1.90%
- **Sharpe Ratio**: 0.499
- **Maximum Drawdown**: -10.08%
- **Hit Rate**: 7.8% (overlap between predicted and actual top 5)
- **Win Rate**: 56.2%

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/ArjunDivecha/T2-NN.git
cd T2-NN

# Install dependencies
pip install -r requirements.txt
```

### Usage

```bash
# Run complete system (hyperparameter tuning + backtest + reports)
python main.py

# Run individual components
python src/tune.py          # Hyperparameter tuning only
python src/backtest.py      # Full rolling window backtest
python generate_reports.py  # Generate performance reports

# Run single window analysis
python show_all_predictions.py  # Detailed factor analysis for single month
```

## ğŸ“ Project Structure

```
T2-NN/
â”œâ”€â”€ src/                     # Core modules
â”‚   â”œâ”€â”€ config.py           # Configuration and hyperparameters
â”‚   â”œâ”€â”€ data.py             # Data loading and preprocessing
â”‚   â”œâ”€â”€ model.py            # Neural network architecture
â”‚   â”œâ”€â”€ train.py            # Training procedures
â”‚   â”œâ”€â”€ backtest.py         # Rolling window backtesting
â”‚   â””â”€â”€ evaluate.py         # Performance evaluation
â”œâ”€â”€ data/                   # Input data files
â”‚   â”œâ”€â”€ T60.xlsx           # Factor forecasts (83 factors)
â”‚   â””â”€â”€ T2_Optimizer.xlsx  # Actual returns
â”œâ”€â”€ outputs/                # Results and reports
â”‚   â”œâ”€â”€ T60_Enhanced_NN.xlsx        # Enhanced forecasts
â”‚   â”œâ”€â”€ Performance_Report.pdf     # Executive summary
â”‚   â”œâ”€â”€ Performance_Report.xlsx    # Detailed analysis
â”‚   â””â”€â”€ plots/             # Visualization charts
â”œâ”€â”€ main.py                # Main execution script
â”œâ”€â”€ generate_reports.py    # Report generation
â””â”€â”€ requirements.txt       # Python dependencies
```

## ğŸ“ˆ Key Features

### Custom Loss Function
- **Top-5 Return Loss**: Optimizes for the average return of the top 5 predicted factors
- **Temperature-scaled softmax**: Smooth selection mechanism for differentiable training
- **Rolling window validation**: Prevents temporal leakage

### Robust Backtesting
- **242 monthly predictions**: Feb 2005 - Mar 2025
- **60-month training windows**: Uses all available historical data
- **Out-of-time validation**: Strict temporal ordering prevents look-ahead bias
- **Comprehensive metrics**: Return, risk, and attribution analysis

### Professional Reporting
- **PDF Executive Summary**: Clean, professional performance report
- **Excel Workbook**: Detailed analysis with multiple sheets
- **Visualization Charts**: Performance, risk, and factor attribution plots
- **Factor Analysis**: Selection frequency and contribution analysis

## ğŸ”§ Technical Details

### Data Requirements
- **Input Format**: Excel files with 83 factors and monthly observations
- **Date Range**: Minimum 60 months for initial training window
- **Data Validation**: Automatic alignment and missing value checks

### Model Training
- **Early Stopping**: Prevents overfitting with patience mechanism
- **Batch Training**: Configurable batch sizes for memory efficiency
- **Reproducible Results**: Fixed random seeds for consistent outputs
- **Device Support**: MPS (Apple Silicon) and CPU training

### Performance Metrics
- **Return Metrics**: Total, monthly, and annualized returns
- **Risk Metrics**: Volatility, Sharpe ratio, maximum drawdown
- **Model Metrics**: Hit rate, win rate, factor turnover
- **Attribution**: Individual factor contribution analysis

## ğŸ“Š Output Files

### Primary Deliverables
- **T60_Enhanced_NN.xlsx**: Enhanced factor forecasts (same format as input)
- **Performance_Report.pdf**: Executive summary with key metrics
- **Performance_Report.xlsx**: Comprehensive analysis workbook

### Supporting Analysis
- **backtest_results.csv**: Monthly backtest details
- **Factor attribution charts**: Selection frequency and performance
- **Risk analysis plots**: Drawdowns, volatility, rolling metrics

## âš™ï¸ Configuration

Key hyperparameters can be modified in `src/config.py`:

```python
DEFAULT_CONFIG = {
    'hidden_sizes': [512, 256],
    'learning_rate': 0.001,
    'dropout_rate': 0.4,
    'batch_size': 32,
    'weight_decay': 1e-05
}

TRAINING_CONFIG = {
    'n_epochs': 100,
    'early_stopping_patience': 10,
    'random_seed': 42
}
```

## ğŸ”¬ Methodology

### Rolling Window Approach
1. **Training**: Use 60 months of historical data
2. **Prediction**: Forecast returns for next month
3. **Selection**: Choose top 5 factors based on predictions
4. **Evaluation**: Compare with actual top 5 performers
5. **Roll Forward**: Move window by 1 month and repeat

### Custom Loss Function
The model uses a differentiable approximation of top-5 selection:
- Temperature-scaled softmax creates smooth factor weights
- Loss maximizes expected return of weighted factor portfolio
- Enables gradient-based optimization of discrete selection problem

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ¤ Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“§ Contact

Arjun Divecha - [@ArjunDivecha](https://github.com/ArjunDivecha)

Project Link: [https://github.com/ArjunDivecha/T2-NN](https://github.com/ArjunDivecha/T2-NN)